{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "94801d77",
      "metadata": {
        "id": "94801d77"
      },
      "source": [
        "# GENERATIVE-TEXT-MODEL\n",
        "\n",
        "This notebook demonstrates how to generate coherent paragraphs on specific topics using GPT-2 (a transformer-based model) or an LSTM-based model.\n",
        "We use the Hugging Face Transformers library for GPT-2 and TensorFlow/Keras for a simple LSTM example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zNuUMMMfTf_"
      },
      "source": [
        "### 1. Install Required Libraries"
      ],
      "id": "_zNuUMMMfTf_"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ddb4450b",
      "metadata": {
        "id": "ddb4450b"
      },
      "outputs": [],
      "source": [
        "!pip install transformers tensorflow --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmzMmGT3figO"
      },
      "source": [
        "### 2. GPT-2 Text Generation Demo"
      ],
      "id": "nmzMmGT3figO"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6ad2b155",
      "metadata": {
        "id": "6ad2b155"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def gpt2_generation(prompt, max_length=200, num_return_sequences=1):\n",
        "    \"\"\"\n",
        "    Generates text using a pretrained GPT-2 model.\n",
        "    Args:\n",
        "        prompt (str): Input text prompt.\n",
        "        max_length (int): Maximum length of generated text.\n",
        "        num_return_sequences (int): Number of variations to generate.\n",
        "    Returns:\n",
        "        list[str]: Generated text sequences.\n",
        "    \"\"\"\n",
        "    generator = pipeline('text-generation', model='gpt2')\n",
        "    outputs = generator(prompt, max_length=max_length, num_return_sequences=num_return_sequences)\n",
        "    return [out['generated_text'] for out in outputs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "32b66035",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32b66035",
        "outputId": "68d3ace1-8c57-4141-c19d-d4d47d252139"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-2 Generated Text:\n",
            " The future of artificial intelligence in healthcare is still far from clear. There are some hints there are a few promising models, but at the moment it's far too early to talk about them with certainty, or make predictions about them with confidence that anything new will ever happen. But at the moment it still appears that the next big question will be that which future of artificial intelligence has the most potential, and which should be expected fairly soon, and which is what we're looking at next.\n",
            "\n",
            "It would probably be a useful analogy to a person in a game, who knows what happens next in the future, but who is most capable of answering the question of (should) the future of artificial intelligence. If everyone in the game is capable of answering\n"
          ]
        }
      ],
      "source": [
        "prompt = \"The future of artificial intelligence in healthcare is\"\n",
        "generated = gpt2_generation(prompt, max_length=150)\n",
        "print(\"GPT-2 Generated Text:\\n\", generated[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yi87mMRfwzn"
      },
      "source": [
        "### 3. LSTM-based Text Generation Demo"
      ],
      "id": "5yi87mMRfwzn"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "41e3a09d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41e3a09d",
        "outputId": "47460d50-d198-40c4-8158-875557b7032a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM Generated Text:\n",
            " Transformers outperform rnns on many nlp tasks tasks tasks tasks tasks\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"Natural language processing enables computers to understand human language.\",\n",
        "    \"Deep learning methods have revolutionized AI research.\",\n",
        "    \"Transformers outperform RNNs on many NLP tasks.\"\n",
        "]\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_seq = token_list[:i+1]\n",
        "        sequences.append(n_gram_seq)\n",
        "\n",
        "# Pad sequences and create predictors & label\n",
        "data = pad_sequences(sequences, padding='pre')\n",
        "X, labels = data[:,:-1], data[:,-1]\n",
        "y = tf.keras.utils.to_categorical(labels, num_classes=len(tokenizer.word_index)+1)\n",
        "\n",
        "# Build LSTM Model\n",
        "def build_lstm(vocab_size, seq_len):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, 50, input_length=seq_len))\n",
        "    model.add(LSTM(100))\n",
        "    model.add(Dense(vocab_size, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "seq_len = X.shape[1]\n",
        "model = build_lstm(vocab_size, seq_len)\n",
        "\n",
        "# Train model\n",
        "epochs = 100\n",
        "model.fit(X, y, epochs=epochs, verbose=0)\n",
        "\n",
        "# Text generation with LSTM\n",
        "def generate_text_lstm(seed_text, next_words, max_seq_len):\n",
        "    \"\"\"\n",
        "    Generates text using the trained LSTM model.\n",
        "    Args:\n",
        "        seed_text (str): Initial text to start generation.\n",
        "        next_words (int): Number of words to generate.\n",
        "        max_seq_len (int): Maximum sequence length for padding.\n",
        "    Returns:\n",
        "        str: Generated text.\n",
        "    \"\"\"\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_seq_len, padding='pre')\n",
        "        predicted = model.predict(token_list, verbose=0)\n",
        "        predicted_word_index = np.argmax(predicted, axis=1)[0]\n",
        "        output_word = ''\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_word_index:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += ' ' + output_word\n",
        "    return seed_text\n",
        "\n",
        "# Example LSTM generation\n",
        "generated_lstm = generate_text_lstm(\"Transformers\", next_words=10, max_seq_len=seq_len)\n",
        "print(\"LSTM Generated Text:\\n\", generated_lstm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCnRU2cSgX6G"
      },
      "source": [
        "### 4. User prompt-driven generation function"
      ],
      "id": "HCnRU2cSgX6G"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_on_topic(topic, model_type='gpt2', **kwargs):\n",
        "    \"\"\"\n",
        "    Generate a paragraph on a specific topic using selected model.\n",
        "    Args:\n",
        "        topic (str): The topic prompt.\n",
        "        model_type (str): 'gpt2' or 'lstm'.\n",
        "    Returns:\n",
        "        str: Generated paragraph.\n",
        "    \"\"\"\n",
        "    if model_type == 'gpt2':\n",
        "        return gpt2_generation(topic, **kwargs)[0]\n",
        "    elif model_type == 'lstm':\n",
        "        # choose a fixed small number of words\n",
        "        return generate_text_lstm(topic, next_words=50, max_seq_len=seq_len)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported model type: choose 'gpt2' or 'lstm'.\")\n",
        "\n",
        "# Demo on user prompt\n",
        "user_topic = \"Climate change and sustainable energy\"\n",
        "print(\"Generated Paragraph on User Topic (GPT-2):\\n\", generate_on_topic(user_topic, model_type='gpt2', max_length=200))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrQQPPg0em7J",
        "outputId": "aa14f567-dac9-4f6d-86c7-0bd85e8d40b0"
      },
      "id": "WrQQPPg0em7J",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Paragraph on User Topic (GPT-2):\n",
            " Climate change and sustainable energy management, both in the form of carbon taxes and other mitigation policies, have all benefited local energy companies and helped create jobs. They are also supporting growth in renewable technologies, which enable local businesses to create, store and charge their own energy resources.\n",
            "\n",
            "The impact of these three measures is vast. In 2013, the Australian Energy Market Operator (AEMO) registered a gain of 5.6%. However, in 2014-2015, the AEMO registered a loss of 6.4%.\n",
            "\n",
            "Australian Energy Market Operator's impact on regional business, particularly coal mining, is a complex task. While Australian businesses are increasingly involved in international trade, they are constrained by Australian regulations. This means international competition and industry must be held accountable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fxT4gBpugjcJ"
      },
      "id": "fxT4gBpugjcJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}